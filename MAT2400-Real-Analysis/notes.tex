\input{preamble.tex}

\title{\huge{Real Analysis}}
\author{\LARGE{Thobias Høivik}}
\date{\Large{Spring 2026}}

\begin{document}
\maketitle

\newpage
\tableofcontents

\newpage
\section{Introduction}
The following is intended for anyone who stumbles over these notes. 
This is intended to be my personal notes in real analysis. 
I will hopefully be attending MAT2400 Real Analysis at 
the University of Oslo in the spring of 2026. However, 
as I am not a program student at this institution, but 
just someone who takes individual courses there 
of my own volition, I do not and cannot attend lectures and 
therefore I have to learn the material on my own. 
As far as I understand, while this course is called Real Analysis, 
it is a bit different than a first course from what I understand. 
The earlier exams give a hint of functional analysis and 
also include topics such as fourier analysis, meassure- and integration
theory among other things. Thus the different supplementary 
coursematerial I will use to aid myself in learning the content 
of this course will most likely be a bit scattered and all over the 
place, which these notes will undoubtedly reflect. I will 
do my best to keep things organized for my own sake, but keep 
this in mind if you are someone who intends to use these 
notes to learn Real Analysis. 

\newpage 
\section{Basic Banch Space theory}
The following section of notes is derived from the first video 
in the lecture series MIT 18.102 Introduction to Functional 
Analysis, Spring 2021 (found on youtube). 

\begin{defn}[Vector Space]
    \label{defn:vector_space}
    A vector space $V$ over a field $\mathbb F$ is a nonempty 
    set of elements called "vectors" together with 
    a binary operation $+$ on $V$ and a binary function 
    $\cdot$ which maps elements of $V, \mathbb F$ to $V$ 
    satisfying: 

    \begin{enumerate}
        \item Associativity of vector addition: 
            $$ 
                u + (v+w) = (u+v)+w, \forall u,v,w \in V
            $$
        \item Commutativity of vector addition: 
            $$ 
                u + v = v + w, \forall u,v \in V
            $$ 
        \item Identity element: 
            $$ 
                \exists 0 \in V : v + 0 = 0 + v = v, \forall v \in V
            $$ 
        \item Each $v \in V$ has an inverse $-v$ under the 
            vector-addition operation.
        \item Scalar multiplication is compatible with field 
            multiplication: 
            $$ 
                a(bv) = (ab)v
            $$ 
            where $a,b \in \mathbb F$ and $v \in V$. 
        \item The multiplicative identity $1 \in \mathbb F$ 
            satisfies: 
            $$ 
                1v = v, \forall v \in V
            $$ 
        \item Distributivity of scalar multiplication with respect 
            to vector addition: 
            $$ 
                a(u+v) = au + av
            $$ 
            where $a \in \mathbb F$ and $u,v \in V$. 
        \item Distributivity of scalar multiplication with 
            respect to field addition: 
            $$ 
                (a+b)v = av + bv
            $$ 
            where $a,b \in \mathbb F$ and $v \in V$. 
    \end{enumerate}

    When proving that something is a vector space, most 
    of these follow naturally from showing closure under 
    addition and scalar multiplication and those two properties, 
    are generally enough to show that it is indeed a vector space. 

    A subspace $U$ of $V$ is a set $U \subseteq V$ which is also 
    a vector space. It is enough to show that $U \subseteq V$ and 
    that it is closed under the two operations. 
\end{defn}

\newpage 
Some typical examples of vector spaces are 
$\mathbb F^n$ where $\mathbb F$ is the reals or the complex 
numbers. We also have spaces like the space of real polynomials 
of degree $\leq n$, i.e. $\mathcal P_n = 
\{\sum_{i=0}^n \alpha_i x^i : \alpha_i \in \mathbb R\}$, 
which is itself a subspace of the space of continuous real-valued 
functions $C(\mathbb R)$. 

So $\mathbb R^2$ and $C(\mathbb R)$ 
are both vector spaces over $\mathbb R$, but 
they have one really big difference, that being the dimension. 

\begin{defn}
    \label{defn:linear_independence}
    Let $V$ be a vector space. A set $\{v_1,\ldots, v_n\} \subseteq V$
    is linearly independent if  
    $$ 
        \displaystyle\sum_{i=1}^n \alpha_i v_i = 0 
        \Leftrightarrow \alpha_1 = \dots = \alpha_n = 0 \in \mathbb F
    $$ 

    Note: the right-to-left direction of this implication is 
    always true. 
\end{defn}

The two spaces discussed above are different in dimension, 
$\mathbb R^2$ being $2$-dimensional and the other being 
infinite-dimensional. 
One definition of finite-dimensional is that every linearly 
independent set in the space is finite. 
I however like the definition using bases more. 
Both of these definitions are equivalent. 

We won't give a rigorous definition of a basis, but in short 
a basis of $V$ is a linearly independent set of vectors which spans 
$V$, i.e. every vector in $V$ can be expressed as a linear combination
of basis-vectors. If the basis is finite then $V$ 
is finite dimensional. Moreover, if the basis is finite then the 
dimension of $V$ is the number of basis-vectors. Note that 
if a finite dimensional space $V$ has a basis with $n$ elements 
then every basis of $V$ has $n$ elements. A space is  
infinite-dimensional if no finite set of linearly independent vectors 
spans the space. 

\newpage 
\subsection{Norms and Metrics}

\begin{defn}[Norm]
    \label{defn:norm}
    Let $V$ be a vector space. 
    A norm $|| \cdot || $ is a function 
    from $V \to [0,\infty)$ satisfying: 
    \begin{enumerate}
        \item $|| v || = 0$ if and only if 
            $v = 0$. 
        \item $|| \alpha v || = |\alpha| \cdot ||v||$ where 
            $\alpha$ is an element of the ground field. 
        \item $||v + w|| \leq ||v|| + ||w||$.
    \end{enumerate}
    
    The tuple $(V, ||\cdot||)$ is called a normed space. 
\end{defn}

\begin{ex}
    \label{ex:lp_norms}
    $||x||_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$ defines 
    a norm on $\mathbb R^n$.

    In fact it constitutes a norm on $\mathbb C^2$ as well. 
    Formally, if we take $\mathbb F = \mathbb R$ or  
    $\mathbb F = \mathbb C$ the p-norm of a vector 
    $v \in \mathbb F^n$ ($p \in [1,\infty]$) is
    $$ 
        ||v||_p := 
        \begin{cases}
            \left(\displaystyle\sum_{i=1}^n |v_i|^p\right)^{1/p} & 
            p < \infty\\ 
            \max_{i=1,\dots,n}|v_i| & p = \infty
        \end{cases}
    $$ 
\end{ex}

Norms give us a notion of the "length" of a vector.  
Now all we need to do analysis on spaces is a notion of 
distance. Intuitively, norms already give us a notion of 
distance from $0$. 

\begin{defn}
    \label{defn:metric}
    Let $X$ be a set. 

    A metric is a function 
    $d:X^2 \to [0,\infty)$ satisfying: 
    \begin{enumerate}
        \item $d(x,y) = 0$ if and only if $x = y$. 
        \item $d(x,y) = d(y,x)$. 
        \item $d(x,z) \leq d(x,y) + d(y,z)$. 
    \end{enumerate}
\end{defn}

The metric gives us a notion of distance. 
In a typical first course in analysis where we work on 
the reals, $d(a,b) = |a - b|$ is the metric we deal with. 

\newpage 
\begin{prop}
    Let $V$ be a normed space with norm $||\cdot||$. 
    Then we can define the distance (a metric) between two vectors 
    by 
    $$ 
        d(x,y) := \left|\left| x - y \right|\right|
    $$ 

    In other words you can define a metric in terms of the norm 
    in any normed space. This metric is usually refered to 
    as the metric induced by the norm. 
\end{prop}

We won't provide a proof of this as it's fairly intuitive. 
Now we can get a sense of convergence and continuity in 
vector spaces by saying that a sequence $\{a_n\}_{n \in \mathbb N}$
converges to a value $a$ if 
$$ 
    \forall \varepsilon > 0, \exists N \in \mathbb N : 
    n \geq N \Rightarrow ||a_n - a || < \varepsilon 
$$ 
and a linear transformation (look up definition if necessary) 
$T \in \mathcal L(U,V)$ is (uniformly) continuous if 
$$ 
    \forall \varepsilon > 0, \exists \delta > 0 : 
    ||x - y||_U < \delta \Rightarrow 
    ||Tx - Ty||_V < \varepsilon
$$ 
for every $x,y \in U$. Notice that if you replace $||x-y||$ with 
$d(x,y)$ it looks like the standard definitions in terms 
of metric spaces. 

\newpage 
\subsection{Banach Spaces}
\begin{defn}[Banach Space]
    \label{defn:banach_space}
    A normed space $V$ is a Banach Space if it is 
    complete with respect to the metric induced by the norm, meaning
    that every cauchy sequence converges to a value in the 
    space. 
\end{defn}

\begin{ex} 
    $\mathbb R^n$ or $\mathbb C^n$ form Banach Spaces 
    with respect to the $\ell^p$ norms (see \Cref{ex:lp_norms}).
\end{ex}

\begin{thm}
    If $X$ is a complete metric space, then 
    $C_\infty(X)$ is a Banach Space. 
\end{thm}

Recall that $C_\infty(X)$ is the space of bounded continuous 
functions on $X$.

\begin{proof}
    We show that every Cauchy sequence in $C_\infty(X)$ converges 
    to an element
    of $C_\infty(X)$.

    Let $\{u_n\}_{n=1}^\infty \subseteq C_\infty(X)$ be a Cauchy 
    sequence with
    respect to the supremum norm. Then for every $\varepsilon > 0$ 
    there exists
    $N \in \mathbb{N}$ such that
    \[
        \|u_n - u_m\|_\infty < \varepsilon \quad \text{for all } 
        n,m \ge N.
    \]
    Equivalently,
    \[
        |u_n(x) - u_m(x)| < \varepsilon
        \quad \text{for all } x \in X \text{ and all } n,m \ge N.
    \]

    Fix $x \in X$. Then $\{u_n(x)\}_{n=1}^\infty$ is a 
    Cauchy sequence in
    $\mathbb{R}$ (or $\mathbb{C}$), since
    \[
        |u_n(x) - u_m(x)| \le \|u_n - u_m\|_\infty.
    \]
    Because $\mathbb{R}$ (or $\mathbb{C}$) is complete, the limit
    \[
        u(x) := \lim_{n\to\infty} u_n(x)
    \]
    exists. This defines a function $u : X \to \mathbb{R}$.

    We now show that $u_n \to u$ uniformly on $X$. Let 
    $\varepsilon > 0$ and
    choose $N$ such that $\|u_n - u_m\|_\infty < \varepsilon$ 
    for all $n,m \ge N$.
    Fix $n \ge N$ and $x \in X$. Taking the limit $m \to \infty$ gives
    \[
        |u_n(x) - u(x)|
        = \lim_{m\to\infty} |u_n(x) - u_m(x)|
        \le \varepsilon.
    \]
    Since $x \in X$ was arbitrary, it follows that
    \[
        \|u_n - u\|_\infty \le \varepsilon \quad \text{for all } 
        n \ge N.
    \]
    Thus $u_n \to u$ uniformly on $X$.

    Since each $u_n$ is bounded and the convergence is uniform, 
    the limit
    function $u$ is bounded. Moreover, since each $u_n$ 
    is continuous and
    uniform limits of continuous functions are continuous, 
    $u$ is continuous
    on $X$.

    Therefore $u \in C_\infty(X)$ and $\{u_n\}$ converges to $u$ in the
    supremum norm. Hence $C_\infty(X)$ is complete, 
    and thus a Banach space.
\end{proof}

\newpage 
\section{A step back into Calculus Theory}
The following section is largely in accordance with 
chapter 2 of Lindstrøm's Spaces: An Introduction 
To Real Analysis. 

\begin{defn}[Limit of a sequence]
    \label{defn:limit_sequence}
    Let $(x_n)$ be a sequence of real numbers and $x \in \mathbb R$. 

    We say 
    $$ 
        x_n \to x
    $$ 
    if for every $\varepsilon > 0$, there exists a natural number 
    $N$ such that 
    $$ 
        |x_n - x| < \varepsilon, \; \forall n \geq N
    $$ 
\end{defn}
This is among the most important definitions in a first 
course in Real-Analysis.   
Intuitively what it captures is the sense of being able 
to get arbitrarily close to a value. 
We will all be familiar with examples from 
calculus like $\lim_{n\to\infty} \frac{1}{n} = 0$. 

\begin{thm}
    \label{thm:limit_unique}
    If a sequence converges, then its limit is unique. 
\end{thm}
\begin{proof}
    Suppose, for a contradiction, that we have some sequence 
    $(x_n)$ with 
    $$ 
        x_n \to x \text{ and } x_n \to y
    $$ 
    of course with $x \neq y$.

    $x_n$ converges so we can choose any $\varepsilon > 0$ 
    for the condition in \cref{defn:limit_sequence} to hold. 

    Consider the case of $\varepsilon = \frac{|x-y|}{2}$. 
    $\varepsilon > 0$ since $x \neq y$ and is therefore a value 
    for which the condition must hold. Namely, 
    $$ 
        |x_n - x| < \varepsilon \text{ and } |x_n - y| < \varepsilon
    $$ 
    leading to
    $$ 
        |x - x_n| + |x_n - y| < 2\varepsilon = |x - y|
    $$ 
    which, via triangle inequality, yields 
    $$ 
        |x-y| = |x - x_n + x_n - y| \leq |x - x_n| + |x_n - y| < |x-y| 
    $$ 
    in particular 
    $$ 
        |x-y| < |x-y|
    $$ 
    which is not possible, a contradiction. 

\end{proof}

\newpage 
\begin{thm}[Algebra of Limits]
    \label{thm:algebra_limits}
    If $x_n \to x$ and $y_n \to y$, then: 
    \begin{enumerate}
        \item $x_n + y_n \to x + y$ 
        \item $x_ny_n \to xy$
        \item If $y_n \neq 0$ and $y \neq 0$, then $\frac{x_n}{y_n}
            \to \frac{x}{y}$.
    \end{enumerate}
\end{thm}
\begin{proof}[Proof of (1): Sum of Limits]
    Let $(x_n)$ and $(y_n)$ be convergent sequences with 
    $x_n \to x$ and $y_n \to y$. 

    Let $\varepsilon > 0$. Since both sequences converge to their 
    respective limits we have some 
    $N_1, N_2 \in \mathbb N$ such that 
    \begin{align*}
        |x_n - x| &< \varepsilon/2, \: \forall n \geq N_1 \\ 
        |y_n - y| &< \varepsilon/2, \: \forall n \geq N_2
    \end{align*}

    Let $N = \max\{N_1, N_2\}$, then $n \geq N$ satisfies 
    $n \geq N_1$ and $N \geq N_2$ so 
    $$ 
        |x_n - x| + |y_n - y| < \varepsilon
    $$ 

    Notice that via the triangle inequality we get 
    $$ 
        |(x_n + y_n) - (x + y)| = |(x_n - x) + (y_n - y)| 
        \leq |x_n - x| + |y_n - y| < \varepsilon
    $$ 
    completing the proof. 
\end{proof}

\begin{thm}
    A sequence $x_n$ converges to $x$ if and only if 
    $$
        \lim \sup x_n = \lim \inf x_n = x
    $$ 
\end{thm}
We will take this theorem without proof as it requires some background
about monotone sequences and completeness which are covered 
later. 

\begin{defn}[Limit of a function]
    \label{defn:limit_function}
    Let $f: D \subset \mathbb R \to \mathbb R$, and let 
    $a$ be a limit point of $D$. 

    We say 
    $$ 
        \lim_{x \to a} f(x) = L 
    $$ 
    if for every $\varepsilon > 0$ there exists $\delta > 0$ 
    such that 
    $$ 
        0 < |x - a| < \delta \Rightarrow |f(x) - L| < \varepsilon
    $$ 
\end{defn}

Later, when we get to metric spaces again, this condition becomes 
$$ 
    d_X(x,a) < \delta \Rightarrow d_Y(f(x), f(a)) < \varepsilon
$$ 
for continuity. 

\begin{thm}[Sequential criterion]
    $$ 
        \lim_{x\to a}f(x) = L \Leftrightarrow 
        \text{for every sequence } x_n \to a \text{ with }
        x_n \neq a, f(x_n) \to L
    $$ 
\end{thm}
\begin{proof}[Proof sketch]
    We only prove $\Rightarrow$, for now. 

    Assume $\lim_{x\to a}f(x) = L$. 

    Let $x_n \to a$ with $x_n \neq a$. 

    Given $\varepsilon > 0$, choose $\delta > 0$ from the 
    definition of the limit. 

    Since $x_n \to a$ there exists $N \in \mathbb N$ such that 
    with $n \geq N$ we have 
    $$ 
        |x_n - a| < \delta 
    $$ 

    Hence for $n \geq N$, 
    $$ 
        |f(x_n) - L| < \varepsilon
    $$ 

    Thus $f(x_n) \to L$.
\end{proof}

\newpage 
\section{Completeness}
\begin{defn}[Upper Bound]
    \label{defn:upper_bound}
    A set $A \subset \mathbb R$ is bounded above if 
    there exists $M \in \mathbb R$ such that 
    $$ 
        a \leq M, \: \forall a \in A
    $$ 

    Such an $M$ is called an upper bound. 
\end{defn}

\begin{defn}[Supremum]
    \label{defn:sup}
    Let $A \subset \mathbb R$. 

    A number $s \in \mathbb R$ is the supremum of $A$ if it 
    is an upper bound of $A$ and for any 
    upper bound $u$ of $A$ we have 
    $$ 
        s \leq u
    $$  
    and we use the notation 
    $$ 
        s = \sup A
    $$ 
\end{defn}

The completeness axiom says that every nonempty subset of 
$\mathbb R$ that is bounded above has a supremum in $\mathbb R$, 
this being the distinguishing quality separating 
$\mathbb R$ from $\mathbb Q$. 

Sidenote: You can also characterize the reals 
as the \emph{unique} ordered field containing $\mathbb Q$ which 
has the least upper bound property. More precisely, every 
$\mathbb F$ with this property satisfies $\mathbb F \cong \mathbb R$.

\begin{prop}
    If $A$ has a supremum then for every $\varepsilon > 0$, 
    there exists $a \in A$ such that 
    $$ 
        s - \varepsilon < a \leq s
    $$ 
\end{prop}
\begin{proof}
    Let $\varepsilon > 0$. If no $a \in A$ existed with the 
    desired property, then $s - \varepsilon$ would be a supremum, 
    contradicting minimality of $s$.
\end{proof}
This proposition becomes useful later in many arguments. 

The infimum $\inf A$ is also something we need and classically we 
define it as being the greatest lower bound, but for 
the purposes of keeping things simple, we 
just let the infimum of a set $A$, $\inf A$, be defined as 
$$ 
    \inf A = -\sup(-A)
$$ 

\newpage 
\begin{thm}[Monotone convergence]
    Let $(x_n)$ be a monotone increasing sequence 
    (a sequence with which $x_n \leq x_{n+1}$ for every 
    $n \in \mathbb N$) which is bounded above. 

    Then $(x_n)$ converges, and 
    $$ 
        \lim x_n = \sup\{x_n : n \in \mathbb N\}
    $$ 
\end{thm}
\begin{proof}
    Let 
    $$ 
        A = \{x_n : n \in \mathbb N\}
    $$ 
    
    Since $A$ is bounded above, $A$ has some supremum $s$. 
    We claim $x_n \to s$. 

    Let $\varepsilon > 0$. 

    By the supremum property, there exists $N$ such that 
    $$ 
        s - \varepsilon < x_N < \leq s
    $$ 
    
    Since the sequence is increasing, for all $n \geq N$, 
    $$ 
        s - \varepsilon < x_n \leq s
    $$ 

    Thus 
    $$ 
        |x_n - s| < \varepsilon
    $$ 
\end{proof}

\begin{lem}
    Every cauchy sequence in $\mathbb R$ is bounded. 
\end{lem}
\begin{proof}
    Let $(x_n)$ be cauchy. 

    Choose $\varepsilon = 1$. Then there exists $N$ such 
    that 
    $$ 
        |x_n - x_m| < 1, \: \forall n,m \geq N
    $$ 

    Fix $m = N$. Then for all $n \geq N$: 
    $$ 
        |x_n| \leq |x_N| + |x_n - x_N| < |x_N| + 1
    $$ 
    
    Thus all terms are bounded. 
\end{proof}

\begin{thm}
    Every cauchy sequence in $\mathbb R$ converges. 
\end{thm}
\begin{proof}[Proof idea]
    Let $(x_n)$ be cauchy. 

    $(x_n)$ is bounded. Use the cauchy property to construct nested 
    intervals. Show the intersection of these to be nonempty 
    via completeness of $\mathbb R$. 
    $(x_n)$ will converge to a point in this intersection. 

    A bit more formally, we choose indices $n_1, n_2, \ldots$ 
    such that 
    $$ 
        |x_n - x_m| < 2^{-k}, \: \forall n,m \geq n_k
    $$ 
    and define the intervals 
    $$ 
        I_k = [x_{n_k} - 2^{-k}, x_{n_k} + 2^{-k}]
    $$ 

    Then each $I_k$ is closed and bounded with $I_{k+1} \subset I_k$.
    Completeness wil give us 
    $$ 
        \bigcap_{k \in \mathbb N} I_k \neq \emptyset
    $$ 
    
    There will be an $x$ in this set which is the limit point. 
\end{proof}

\newpage 
\section{Open and Closed Sets in Metric Spaces}
Let $X$ be a set and $A \subseteq X$. 
Intuitively, a point $x$ is either \textbf{inside of} $A$, 
\textbf{outside of} $A$, or \textbf{on the boundary} of A.  
We know what it means to not be in $A$; $a \not\in A \Leftrightarrow 
a \in A^c$. 
We also know what $a \in A$ means, but set-theoretically we don't 
have a notion of being on the "boundary" of $A$. 
We can make sense of this notion in a metric space. 

Recall that the open ball is the set 
$B(x;r) = \{z \in X : d(x,z) < r\}$. Likewise we can define 
the closed ball as follows. 

\begin{defn}[Closed Ball]
    The closed ball centered at $x \in X$ with radius $r \geq 0$ in 
    $B(x;r) = \{z \in X : d(x,z) \leq r\}$. 
\end{defn}

Let $(X,d)$ be a metric space with $A \subseteq X$. 

\begin{itemize}
    \item $x \in X$ is an interior point of $A$ if 
        $B(x;r) \subseteq A$ for some $r > 0$. 
    \item $y \in X$ is an interior point of $A$ if 
        $B(x;r) \subseteq A^c$ for some $r > 0$. 
    \item $z \in X$ is a boundary point of $A$ if it is neither of 
        the above, i.e. 
        $B(z;r) \cap A \neq \emptyset$ and $B(z;r) \cap A^c \neq  
        \emptyset$ for every $r > 0$.
\end{itemize}

Notably, every point in $X$ is one of these three. 

\begin{itemize}
    \item $A^0 = \{\text{all interior points of } A\}$
    \item $\partial A = \{\text{all boundary points of } A\}$ 
    \item $\overline A = A \cup \partial A = ((A^c)^0)^c$
\end{itemize}

\begin{prop}
    For any $A \subset X$, we have $\partial A = \partial (A^c)$.
\end{prop}

\begin{thm}
    Useful characterizations of openness: 

    $A \subseteq X$ is open if $A = A^0$

    $A \subseteq X$ is open if $A$ contains none of its boundary 
    points. 

    $A \subseteq X$ is open if $A \cap \partial A = \emptyset$

    $A \subseteq X$ is open if $\forall x \in A$ there is some 
    $r > 0$ s.t. $B(x;r) \subseteq A$ 
\end{thm}

\begin{thm}
    Useful characterizations of openness: 

    $B \subseteq X$ is closed if $B = \overline B$

    $B \subseteq X$ is closed if $A$ contains all of its boundary 
    points. 

    $A \subseteq X$ is closed if $ \partial B\subseteq B$
\end{thm}

\newpage 
\begin{prob}
    Consider $X = \mathbb R$ with the canonical metric $|\cdot|$. 
    Show that $(a,b)$ is open and $[a,b]$ is open for 
    $a \leq b \in \mathbb R$, and $(a,b]$ is 
    neither open nor closed for $a < b$.
\end{prob}
\begin{proof}
    Let $x \in (a,b)$, i.e. $a < x < b$. Take 
    \[
        r := \min\{x - a, x - b \} > 0
    \]
    Take any $y \in (x-r, x+y)$. 
    Then 
    \[
        y > x - r \geq a, \: y < x + r \leq b
    \]
    Therefore 
    \[
        (x-r, x + r) \subset (a,b)
    \]

    In other words, every $x \in (a,b)$ admits and open ball 
    contained entirely in the interval, so $(a,b)$ is open.

    Notice that $\partial(a,b) = \{a,b\}$ and that 
    $[a,b] = \partial(a,b) \cup (a,b)$ hence $[a,b]$ is the closure 
    of $(a,b)$ and thus is closed.  

    It is clear that $(a,b]$ is neither open nor closed as 
    $\partial(a,b] = \{a,b\} \not \subseteq (a,b]$ and 
    $(a,b] \cap \partial(a,b] \neq \emptyset$. 
\end{proof}

\newpage 
\section{Week 4 problem set}
\begin{prop}[Proposition 3.1.4 in Spaces]
    \label{prop:reverse_triangle}
    If $(X,d)$ is a metric space with $x,y,z \in X$, then 
    \[
        |d(x,y) - d(y,z)| \leq d(x,z)
    \]
\end{prop}
\begin{prob}
    Prove \Cref{prop:reverse_triangle}. 
\end{prob}
\begin{proof}
    Let $(X,d)$ be a metric space and recall the standard 
    triangle inequality: 
    \[
        d(x,z) \leq d(x,y) + d(y,z)
    \]
    Now observe the following rearangements: 
    \begin{align*}
        d(x,y) &\leq d(y,z) + d(x,z) \\ 
        d(x,y) - d(y,z) &\leq d(x,z) \\ 
                        &\text{and} \\ 
        d(x,y) &\leq d(y,z) + d(x,z) \\ 
        d(y,z) &\leq d(x,y) + d(x,z) \\ 
        d(y,z) - d(x,z) &\leq d(x,y) \\ 
        -d(x,z) &\leq d(x,y) - d(y,z) 
    \end{align*}

    Together: 
    \[
        |d(x,y) - d(y,z)| \leq d(x,z)
    \]
\end{proof}
\begin{prob}[3.1.6 Spaces]
    Let $(V,||\cdot||)$ be a normed space. Show that 
    it induces a norm, i.e. 
    \[
        d(x,y) := ||x-y||
    \]
    is a norm.
\end{prob}
\begin{proof}
    Let $(V,||\cdot||)$ be a normed space. 

    \textbf{Non-negativity and Identity of Indiscernibles:}

    \[
        d(x,y) = ||x-y|| \geq 0 
    \]
    since $x-y \in V$ and $||\cdot||$ is non-negative. 
    
    \[
        d(x,x) = ||x-x|| = ||0|| = 0
    \]
    Assume $d(x,y) = 0$. 
    \[
        d(x,y) = ||x-y|| = 0
    \]
    Then $x-y = 0 \Rightarrow x = y$.

    \textbf{Symmetry:}

    Recall for a norm we have $||\alpha x|| = |\alpha| ||x||$, 
    hence 
    \[
        d(x,y) = |1|||x-y|| = ||-1(x-y)|| = ||y-x|| = d(y,x)
    \]

    \textbf{Triangle Inequality:}
    \begin{align*}
        d(x,z) &= ||x-z|| = ||x - (y + y) - z|| \\ 
               &\leq ||x-y|| + ||y - z|| \\ 
               &= ||x-y|| + ||z-y|| \\ 
               &= d(x,y) + d(y,z)
    \end{align*}
\end{proof}

\begin{prob}[3.1.7 Spaces]
    Show that if $x_1, x_2, \ldots, x_n$ are points in a metric space,
    then 
    \[
        d(x_1,x_n) \leq \displaystyle\sum_{i=1}^{n-1} d(x_i, x_{i+1})
    \]
\end{prob}
\begin{proof}
    We proceed by induction on $n \in \mathbb N$. 

    \textbf{Base Case ($n=1$).}

    Clearly, 
    \[
        d(x_1, x_1) \leq d(x_1, x_1)
    \]

    \textbf{Hypothesis.}
    Now assume, for any $k \in \mathbb N$, 
    \[
        d(x_1, x_k) \leq \displaystyle\sum_{i=1}^{k-1}d(x_i, x_{i+1})
    \]

    \textbf{Induction.}
    Let $n = k+1$. 

    \begin{align*}
        d(x_1, x_n) = d(x_1, x_{k+1}) &\leq d(x_1, x_k) + 
        d(x_k, x_{k+1}) \\ 
    &\leq \left(\displaystyle\sum_{i=1}^{k-1} d(x_i, x_{i+1})\right) 
    + d(x_k, x_{k+1}) \\ 
    &= \displaystyle\sum_{i=1}^{n-1} d(x_i, x_{i+1})
    \end{align*}
    as desired. 
\end{proof}

\begin{prob}[3.2.1 Spaces]
    Assume that $(X,d)$ is a discrete metric space. Show that 
    the sequence $\{x_n\}$ converges to $a$ if and only if there is an 
    $N \in \mathbb N$ such that $x_n = a$ for all $n \geq N$. 
\end{prob}
\begin{proof}
    Let $(X,d)$ be a discrete metric space, i.e. a metric space where
    \[
        d(x,y) = 
        \begin{cases}
            0 & \text{if } x = y \\ 
            1 & \text{otherwise}
        \end{cases}
    \]

    \textbf{$\Leftarrow$.} 
    
    Assume there is some $N \in \mathbb N$ such that 
    $\forall n \geq N$, 
    \[
        x_n = a
    \]
    Then it is clear that for any $\varepsilon > 0$, 
    \[
        x_n = 0 \Rightarrow  d(x_n,a) = 0 < \varepsilon
    \]
    so $\{x_n\} \to a$. 

    \textbf{$\Rightarrow$.}

    Assume that $\{x_n\} \to a$. Then for any $\varepsilon > 0$ 
    there exists $N \in \mathbb N$ such that for $n \geq N$, then 
    \[
        d(x_n, a) < \varepsilon
    \]
    In particular, this must hold for $\varepsilon = 0.5$. 
    Notice that this requires $x_n = a$ since otherwise we would 
    have $1 < 0.5$, a contradiction. 
\end{proof}

\begin{prob}[3.2.5 Spaces]
    Let $(X,d)$ be a metric space and fix some $a \in X$. 
    Show that the function $f:X \to \mathbb R$ defined as 
    \[
        f(x) = d(x,a)
    \]
    is continuous. 
    Note: $\mathbb R$ is finite dimensional so we are free 
    to choose any metric. $d{\mathbb R}(x,y) = |x-y|$ is fine.
\end{prob}
\begin{proof}
    Let $(X,d)$ be a metric space and fix $a \in X$. 
    Let $f$ be defined as above. 

    Consider a family of sequences in $X$ such that every sequence 
    $\{x_n\}$ converges to $a \in X$. Let $\{x_n\}$ be some arbitrary
    sequence in this family.

    Then, $\forall \varepsilon > 0$ we have some natural number 
    $\mathbb N$, s.t. for any $n \geq N$,
    \[
        d(x_n, a) < \varepsilon
    \]

    Notice that 
    \[
        d(x_n,a) = |d(x_n,a) - 0| < \varepsilon
    \]
    where $\varepsilon > 0$ was arbitrary. Hence 
    $f(\{x_n\})$ converges to $0 = d(a,a) = f(a)$. 

    Thus $f$ is continuous.
\end{proof}
\begin{prob}
    Let $(X,d)$ be a metric space. Prove that every finite 
    subset of $X$ is closed. 
\end{prob}
\begin{proof}
    Recall that proposition $3.3.13$ from Spaces tells us that 
    for any finite collection $F_1, \ldots, F_n$ of closed sets, 
    their union 
    \[
        \bigcup_{i \in [n]} F_i
    \]
    is closed. 

    Consider any singleton $\{x\} \subseteq X$. It is clear that 
    $\{x\}$ is closed since $\{x\}^c$ is open. Observe that 
    the open ball $B(y;\varepsilon) \subseteq \{x\}^c$ for any 
    $y \in X \setminus \{x\}$ with $\varepsilon = d(x,y)$.
    
    Let $\{x_1,\ldots,x_n\} \subseteq X$ be any finite subset. As 
    shown above, $\{x_i\}$ is closed, and by proposition 3.3.13 
    \[
        \{x_1,\ldots,x_n\} = \bigcup_{i\in[n]} \{x_i\}
    \] 
    is closed. 
\end{proof}

\begin{prob}[3.3.13 Spaces]
    A metric space $(X,d)$ is disconnected if it is the union 
    of two non-empty, disjoint and open subsets. If it is not 
    disconnected it is connected. 
    
    a) Let $X = (-1,1) \setminus \{0\}$ and let $d$ be the usual 
    metric on $X$. Show that $(X,d)$ is disconnected. 

    b) Let $X = \mathbb Q$ and let $d$ be the usual metric again. 
    Show that $(X,d)$ is disconnected. 

    c) Assume that $(X,d)$ is a connected metric space
    and that $f: X \to Y$ is continuous and surjective. Show that 
    $Y$ is connected.
\end{prob}
\begin{proof}[Proof of (a)]
    It is clear that $(-1,0)$ and $(0,1)$ are open with respect to 
    $d(x,y) = |x-y|$ as, we can find an open ball around any 
    point in either. Since 
    \[
        X = (-1,0) \cup (0,1)
    \]
    we conclude that $X$ is disconnected. 
\end{proof}
\begin{proof}[Proof of (b)]
    Recall that $\sqrt 2 \not\in \mathbb Q$ so 
    the open subsets 
    \[
        (-\infty, \sqrt 2) \cap \mathbb Q 
        \text{ and }
        (\sqrt 2, \infty) \cap \mathbb Q 
    \]
    Both of these sets are open in $\mathbb R$ and thus they 
    are also open in $\mathbb Q$.
\end{proof}
\begin{proof}[Proof of (c)]
\end{proof}

\end{document}
